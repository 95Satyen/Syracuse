{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "- TAs: Tong Zeng <tozeng@syr.edu>, Priya Matnani <psmatnan@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image that you are an avid fan of the Chicago Cubs. Somehow, you managed to be part of two email lists, one about baseball (which you are interested in) and hockey (which you are not that interested in). You will use the power of data science to create a classifier that takes an email as an input and predicts whether the email is about baseball or hockey\n",
    "\n",
    "The dataset will be in `email_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa246e0ba90446b952588ca31d716723",
     "grade": false,
     "grade_id": "cell-6d4e17df71a2afa8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "email_df = spark.read.json('emails.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball email\n",
      "============================================\n",
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
      "and many Toronto fans make the trip to Cleveland as it is easier to\n",
      "get tickets in Cleveland than in Toronto.  Either way, I seriously\n",
      "doubt they will sell out until the end of the season.)\n",
      "\n",
      "-- \n",
      "Doug Bank                       Private Systems Division\n",
      "dougb@ecs.comm.mot.com          Motorola Communications Sector\n",
      "dougb@nwu.edu                   Schaumburg, Illinois\n",
      "dougb@casbah.acns.nwu.edu       708-576-8207                    \n",
      "\n",
      "Hockey email\n",
      "============================================\n",
      "From: gld@cunixb.cc.columbia.edu (Gary L Dare)\n",
      "Subject: Re: Flames Truly Brutal in Loss\n",
      "Nntp-Posting-Host: cunixb.cc.columbia.edu\n",
      "Reply-To: gld@cunixb.cc.columbia.edu (Gary L Dare)\n",
      "Organization: PhDs In The Hall\n",
      "Distribution: na\n",
      "Lines: 13\n",
      "\n",
      "\n",
      "This game would have been great as part of a double-header on ABC or\n",
      "ESPN; the league would have been able to push back-to-back wins by\n",
      "Le Magnifique and The Great One.  Unfortunately, the only network\n",
      "that would have done that was SCA, seen in few areas and hard to\n",
      "justify as a pay channel. )-;\n",
      "\n",
      "gld\n",
      "--\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~ Je me souviens ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Gary L. Dare\n",
      "> gld@columbia.EDU \t\t\tGO  Winnipeg Jets  GO!!!\n",
      "> gld@cunixc.BITNET\t\t\tSelanne + Domi ==> Stanley\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore the data a bit\n",
    "print(\"Baseball email\\n============================================\")\n",
    "print(email_df.where('target == \"rec.sport.baseball\"').first().email)\n",
    "print(\"Hockey email\\n============================================\")\n",
    "print(email_df.where('target == \"rec.sport.hockey\"').first().email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.1\n",
    "\n",
    "Add a `topic` column to `email_df` to be 1 if `target` is `rec.sport.baseball` and 0 if it is `rec.sport.hockey` and store the result in `email2_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede01a8c8ef80d25d4fa8d9132b6f9c0",
     "grade": false,
     "grade_id": "cell-79e2472845514523",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "email2_df = email_df.withColumn('topic',(fn.col('target')=='rec.sport.baseball').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+-----+\n",
      "|               email|email_id|            target|topic|\n",
      "+--------------------+--------+------------------+-----+\n",
      "|From: dougb@comm....|       1|rec.sport.baseball|    1|\n",
      "|From: gld@cunixb....|       2|  rec.sport.hockey|    0|\n",
      "|From: rudy@netcom...|       3|rec.sport.baseball|    1|\n",
      "|From: monack@heli...|       4|  rec.sport.hockey|    0|\n",
      "|Subject: Let it b...|       5|rec.sport.baseball|    1|\n",
      "|From: mmb@lamar.C...|       6|  rec.sport.hockey|    0|\n",
      "|From: fierkelab@b...|       7|rec.sport.baseball|    1|\n",
      "|From: rvpst2+@pit...|       8|  rec.sport.hockey|    0|\n",
      "|From: smorris@ven...|       9|  rec.sport.hockey|    0|\n",
      "|From: richard@amc...|      10|  rec.sport.hockey|    0|\n",
      "|From: brifre1@ac....|      11|  rec.sport.hockey|    0|\n",
      "|From: dwk@cci632....|      12|  rec.sport.hockey|    0|\n",
      "|From: cub@csi.jpl...|      13|rec.sport.baseball|    1|\n",
      "|From: golchowy@al...|      14|  rec.sport.hockey|    0|\n",
      "|From: krattige@hp...|      15|rec.sport.baseball|    1|\n",
      "|From: CROSEN1@ua1...|      16|rec.sport.baseball|    1|\n",
      "|Subject: Re: quic...|      17|rec.sport.baseball|    1|\n",
      "|From: luriem@alle...|      18|rec.sport.baseball|    1|\n",
      "|From: fls@keynes....|      19|rec.sport.baseball|    1|\n",
      "|Subject: Re: Jewi...|      20|rec.sport.baseball|    1|\n",
      "+--------------------+--------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check your results\n",
    "email2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email2_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ca9ea08f74b120f93fbc878ef4f0532",
     "grade": true,
     "grade_id": "cell-c6048eb3c38d3030",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal(\n",
    "    email2_df.groupBy('topic').count().orderBy('topic').rdd.map(lambda x: x['count']).collect(),\n",
    "    [600, 597]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.2: tfidf feature engineering\n",
    "Create a pipeline that combines a `Tokenizer`, `CounterVectorizer`, and a `IDF` estimator to compute the tfidf vectors of each email. Fit this pipeline and assign the pipeline transformer to a variable `tfidf_pipeline`. The `Tokenizer` step should create a column `words`, the `CounterVectorizer` step should create a column `tf`, and the `IDF` step should create a column `tfidf`. **Use the default parameers of all the estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "521084e2063144fa97a14bef1e965fca",
     "grade": false,
     "grade_id": "cell-ab24110e63d19470",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "Tokenizer = Tokenizer().setInputCol('email').setOutputCol('words')\n",
    "#Tokenizer.transform(email2_df).show()\n",
    "CounterVectorizer = CountVectorizer().setInputCol('words').setOutputCol('tf')\n",
    "IDF = IDF().setInputCol('tf').setOutputCol('tfidf')\n",
    "tfidf_pipeline = Pipeline(stages=[Tokenizer, CounterVectorizer,IDF]).fit(email2_df)\n",
    "#tfidf_pipeline.transform(email2_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "71998580f176786f2247bd666c373fbd",
     "grade": true,
     "grade_id": "cell-cd280a1ccf0f1705",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal([type(s) for s in tfidf_pipeline.stages],\n",
    "                              [feature.Tokenizer, feature.CountVectorizerModel, feature.IDFModel])\n",
    "np.testing.assert_array_equal(len(tfidf_pipeline.transform(email2_df).collect()), 1197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Investigate the fitted pieline above and create a variable `lowest_idf` that contain the set of words with the 5 lowest IDF. **Hint: you must extract the vocabulary from the fitted `CountVectorizer` and the IDF values from the fitted `IDF`, both in the stages of `tfidf_pipeline`. You can put both lists into a Pandas dataframe columns and sort by idf, picking 5 after sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21857b2e5609001af62a485bad4ea454",
     "grade": false,
     "grade_id": "cell-18bb787517bd43b1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#count_vectorizer_transformer = CounterVectorizer.fit(Tokenizer.transform(email2_df))\n",
    "#count_vectorizer_transformer.vocabulary\n",
    "vocabulary = tfidf_pipeline.stages[1].vocabulary\n",
    "idf = tfidf_pipeline.stages[2].idf\n",
    "ans = pd.DataFrame({'word':vocabulary,'idf':idf})\n",
    "x= ans.sort_values(by=['idf'])\n",
    "y = x[0:5]['word'].tolist()\n",
    "lowest_idf= set(y)\n",
    "#lowest_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faade02098f10061ba0107815cc4d81c",
     "grade": true,
     "grade_id": "cell-5493b6e9b55bd45f",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "# it is a set\n",
    "np.testing.assert_equal(type(lowest_idf), set)\n",
    "# it has 5 elements\n",
    "np.testing.assert_equal(len(lowest_idf), 5)\n",
    "# each element is a string\n",
    "np.testing.assert_equal({type(w) for w in lowest_idf}, {str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.3: Compare models\n",
    "\n",
    "Using the following splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89386fe0e5551b31b173749ef8f9d38f",
     "grade": false,
     "grade_id": "cell-2f05175cd6ae7f5c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_df = email2_df.where('email_id < 1197*0.6')\n",
    "validation_df = email2_df.where('email_id >= 1197*0.6 and email_id < 1197*0.9')\n",
    "testing_df = email2_df.where('email_id >= 1197*0.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[718, 359, 120]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[training_df.count(), validation_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Create pipelines where the first stage is the `tfidf_pipeline` created above and the second stage is a `LogisticRegression` model to predict `target` using different regularization parameters ($\\lambda$) and elastic net mixture ($\\alpha$). *Only change the regularization parameters and leave all parameters as default for `LogisticRegression`*. Fit those pipelines to the appropriate data split.\n",
    "\n",
    "1. Logistic regression with $\\lambda=0$ and $\\alpha=0$ (assign the fitted pipeline to `lr_pipeline1`)\n",
    "2. Logistic regression with $\\lambda=0.02$ and $\\alpha=0.2$ (assign the fitted pipeline to `lr_pipeline2`)\n",
    "3. Logistic regression with $\\lambda=0.1$ and $\\alpha=0.4$ (assign the fitted pipeline to `lr_pipeline3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "02b7fefd0dd482900ba644d56b02400a",
     "grade": false,
     "grade_id": "cell-8db1a50673eea1a7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create lr_pipeline1, lr_pipeline2, and lr_pipeline3\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lambda_par = 0\n",
    "alpha_par = 0\n",
    "en_lr1= LogisticRegression().setLabelCol('topic').setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).setMaxIter(100).setElasticNetParam(alpha_par)\n",
    "en_lr2= LogisticRegression().setLabelCol('topic').setFeaturesCol('tfidf').\\\n",
    "         setRegParam(0.02).setMaxIter(100).setElasticNetParam(0.2)\n",
    "en_lr3= LogisticRegression().setLabelCol('topic').setFeaturesCol('tfidf').\\\n",
    "        setRegParam(0.1).setMaxIter(100).setElasticNetParam(0.4)\n",
    "\n",
    "lr_pipeline1 = Pipeline(stages=[tfidf_pipeline,en_lr1]).fit(training_df)\n",
    "lr_pipeline2 = Pipeline(stages=[tfidf_pipeline,en_lr2]).fit(training_df)\n",
    "lr_pipeline3 = Pipeline(stages=[tfidf_pipeline,en_lr3]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd388632281a19de36557ec7a00af21d",
     "grade": true,
     "grade_id": "cell-bc59c22c523a9016",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(type(lr_pipeline1), pipeline.PipelineModel)\n",
    "np.testing.assert_equal(type(lr_pipeline2), pipeline.PipelineModel)\n",
    "np.testing.assert_equal(type(lr_pipeline3), pipeline.PipelineModel)\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline1.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline2.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline3.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Use the evaluator object defined below to compute the area under the curve of your predictors. For example, to compute the area under the curve of pipeline 1 for a dataframe `df`, you would run\n",
    "\n",
    "```python\n",
    "evaluator.evaluate(lr_pipeline1.transform(df))\n",
    "```\n",
    "\n",
    "**You must choose the right data split (dataframe `df`) with the goal of comparing models.**\n",
    "\n",
    "Assign the AUC of the three models to the variables `AUC1`, `AUC2`, and `AUC3`, and and assign the pipeline with the best model to a variable `best_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59b6e015463ca0fe73c5c25072083486",
     "grade": false,
     "grade_id": "cell-44d4a941d4aef83e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "evaluator = evaluation.BinaryClassificationEvaluator(labelCol='topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the AUC on training of the first model should perfect:\n",
    "\n",
    "```\n",
    "evaluator.evaluate(lr_pipeline1.transform(training_df))\n",
    "```\n",
    "\n",
    "```console\n",
    "1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d2b1fd1d3f5e9dbcd1986f7c6486293",
     "grade": false,
     "grade_id": "cell-a9d883059572796b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 AUC:  0.9937903626428214\n",
      "Model 2 AUC:  0.988170640834575\n",
      "Model 3 AUC:  0.972180824639841\n"
     ]
    }
   ],
   "source": [
    "# print the AUC for the three models as follows\n",
    "# print(\"Model 1 AUC: \", evaluator.evaluate(....))\n",
    "# etc\n",
    "# finally, based on these, assign the best validated \n",
    "# model to a variable best_model\n",
    "# YOUR CODE HERE\n",
    "AUC1 = evaluator.evaluate(lr_pipeline1.transform(validation_df))\n",
    "print(\"Model 1 AUC: \",AUC1)\n",
    "AUC2 = evaluator.evaluate(lr_pipeline2.transform(validation_df))\n",
    "print(\"Model 2 AUC: \",AUC2)\n",
    "AUC3 = evaluator.evaluate(lr_pipeline3.transform(validation_df))\n",
    "print(\"Model 3 AUC: \",AUC3)\n",
    "best_model = lr_pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c76a13060b46900aa055920ae5f6a2f",
     "grade": true,
     "grade_id": "cell-29d52d2cec5c8a1e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal([type(AUC1), type(AUC2), type(AUC3)],\n",
    "                             [float, float, float])\n",
    "# AUC less than 1\n",
    "np.testing.assert_array_less([AUC1, AUC2, AUC3], [1, 1, 1])\n",
    "# AUC more than 0.5\n",
    "np.testing.assert_array_less([.5, .5, .5],\n",
    "                            [AUC1, AUC2, AUC3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.4: Choose best model\n",
    "\n",
    "Using the right split and the best model selected before, compute the generalization performance and assign it to a variable `AUC_best`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7780410d49dd0727bea11bcfa2ec75de",
     "grade": false,
     "grade_id": "cell-2d30a81f9c08fc5d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943181818181818"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign to AUC_best the AUC of the best model selected before\n",
    "AUC_best = evaluator.evaluate(best_model.transform(testing_df))\n",
    "AUC_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f01412786cef477f564cec439e3dbe9f",
     "grade": true,
     "grade_id": "cell-dc8fd7d7ce658642",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_approx_equal(AUC_best, \n",
    "                               0.9943181818181818, significant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.5: Inference\n",
    "\n",
    "Use the pipeline 3 fitted above (`lr_pipeline3`) to create a Pandas dataframes that contain the most negative words and the most positive words. In particular, create a dataframe `positive_words` with the columns `word` and `weight` with the top 20 positive words, sorted by descending coefficient. Similarly create a `negative_words` Pandas dataframe with the top 20 negative words where the coefficient are sorted in ascending order. **Hint: follow the `sentiment_analysis.ipynb` notebook in the repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b517fb83f670ea60b1fdb1580356a410",
     "grade": false,
     "grade_id": "cell-9fa9de453a36f8d2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>golchowy@alchemy.chem.utoronto.ca</td>\n",
       "      <td>-0.160207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>nhl.</td>\n",
       "      <td>-0.148710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>contact</td>\n",
       "      <td>-0.141691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>playoff</td>\n",
       "      <td>-0.131673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>olchowy)</td>\n",
       "      <td>-0.129368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  word    weight\n",
       "816  golchowy@alchemy.chem.utoronto.ca -0.160207\n",
       "882                               nhl. -0.148710\n",
       "570                            contact -0.141691\n",
       "236                            playoff -0.131673\n",
       "810                           olchowy) -0.129368"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create positive_words and negative_words pandas dataframe below\n",
    "lr_weights = lr_pipeline3.stages[-1].coefficients.toArray()\n",
    "lr_coeffs_df = pd.DataFrame({'word': lr_pipeline3.stages[0].stages[1].vocabulary, 'weight': lr_weights})\n",
    "negative_words = lr_coeffs_df.sort_values('weight').head(20)\n",
    "positive_words = lr_coeffs_df.sort_values('weight', ascending=False).head(20)\n",
    "#len(lr_coeffs_df)\n",
    "positive_words.head()\n",
    "negative_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be as follows:\n",
    "\n",
    "`positive_words.head()`\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>word</th>\n",
    "      <th>weight</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>860</th>\n",
    "      <td>(edward</td>\n",
    "      <td>0.123131</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>129</th>\n",
    "      <td>baseball</td>\n",
    "      <td>0.107927</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>991</th>\n",
    "      <td>players?</td>\n",
    "      <td>0.092217</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>285</th>\n",
    "      <td>pitching</td>\n",
    "      <td>0.088141</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>969</th>\n",
    "      <td>fischer)</td>\n",
    "      <td>0.061178</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "`negative_words.head()`\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>word</th>\n",
    "      <th>weight</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>816</th>\n",
    "      <td>golchowy@alchemy.chem.utoronto.ca</td>\n",
    "      <td>-0.160207</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>882</th>\n",
    "      <td>nhl.</td>\n",
    "      <td>-0.148710</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>570</th>\n",
    "      <td>contact</td>\n",
    "      <td>-0.141691</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>236</th>\n",
    "      <td>playoff</td>\n",
    "      <td>-0.131673</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>810</th>\n",
    "      <td>olchowy)</td>\n",
    "      <td>-0.129368</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb9d846088455b7ba9b3f30f5fc79cb7",
     "grade": true,
     "grade_id": "cell-5926b249c3d8d910",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_equal(set(positive_words.columns), {'weight', 'word'})\n",
    "np.testing.assert_equal(set(negative_words.columns), {'weight', 'word'})\n",
    "np.testing.assert_approx_equal(positive_words.weight.sum(), 1.1287686331251567, significant=1)\n",
    "np.testing.assert_approx_equal(negative_words.weight.sum(), -1.9525975400776723, significant=1)\n",
    "np.testing.assert_array_less(positive_words.weight.iloc[-1], positive_words.weight.iloc[0])\n",
    "np.testing.assert_array_less(negative_words.weight.iloc[0], negative_words.weight.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Explain in simple terms what the top three positive words and the top three negative words might indicate about the prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2675042a84222d120993ede4d08c5ab",
     "grade": true,
     "grade_id": "cell-ee3d1f6e740758dd",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "When we look at the top 3 positive words, it consist of symbols such as brackets and ?. Simillary the top 3 negative words contain bracket symbol and punctuation. After a closer look, some words do not have meaning and some cannot be interpreted as positive or negative words. There there is a high possibility of over fitting. This might be possible as number of emails is less as compared to number of words.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
