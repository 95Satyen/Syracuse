{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "- TAs: Tong Zeng <tozeng@syr.edu>, Priya Matnani <psmatnan@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load these packages\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import clustering\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql import functions as fn\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Clustering and Latent Dirichlet Allocation\n",
    "\n",
    "I would recommend to follow the notebook `unsupervised_learning.ipynb` first, shared through the IST 718 repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset contains information about the diet of European contries around 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9102f93b73342f27ff5eb15d025c29c1",
     "grade": false,
     "grade_id": "cell-8898c04579b9221e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "protein_df = spark.read.csv('proteindata.txt', sep='\\t', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: (10 pts)\n",
    "\n",
    "Performed a PCA transformation on the `protein_df` dataframe, using all features. Use two principal components. The pipeline transformation should have a standard scaler step where you only center the data before you pass it onto the PCA transformation. Store the pipeline transformation in `pipe_pca`. The transformation should produce a column named `pc` which has the two principal components requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cb982914cbab5d8d552cafcab8a6f41b",
     "grade": false,
     "grade_id": "cell-18ebd4f77c37a8e3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts) Create ds_programs_text_df here\n",
    "\n",
    "pipe_pca = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=['RedMeat','WhiteMeat', 'Eggs','Milk','Fish','Cereals','Starch','Nuts','FruitVeg'],\n",
    "                            outputCol='features')\n",
    "    ,\n",
    "    feature.StandardScaler(withMean=True,withStd=False,\n",
    "                           inputCol='features', outputCol='centered_features')\n",
    "    ,\n",
    "    feature.PCA(k=2, inputCol='centered_features', outputCol='pc')\n",
    "]).fit(protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- RedMeat: double (nullable = true)\n",
      " |-- WhiteMeat: double (nullable = true)\n",
      " |-- Eggs: double (nullable = true)\n",
      " |-- Milk: double (nullable = true)\n",
      " |-- Fish: double (nullable = true)\n",
      " |-- Cereals: double (nullable = true)\n",
      " |-- Starch: double (nullable = true)\n",
      " |-- Nuts: double (nullable = true)\n",
      " |-- FruitVeg: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- centered_features: vector (nullable = true)\n",
      " |-- pc: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#protein_df\n",
    "pipe_pca.transform(protein_df).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the schema of how the transformation should look like:\n",
    "\n",
    "```python\n",
    "pipe_pca.transform(protein_df).printSchema()\n",
    "```\n",
    "\n",
    "```console\n",
    "root\n",
    " |-- Country: string (nullable = true)\n",
    " |-- RedMeat: double (nullable = true)\n",
    " |-- WhiteMeat: double (nullable = true)\n",
    " |-- Eggs: double (nullable = true)\n",
    " |-- Milk: double (nullable = true)\n",
    " |-- Fish: double (nullable = true)\n",
    " |-- Cereals: double (nullable = true)\n",
    " |-- Starch: double (nullable = true)\n",
    " |-- Nuts: double (nullable = true)\n",
    " |-- FruitVeg: double (nullable = true)\n",
    " |-- features: vector (nullable = true)\n",
    " |-- centered_features: vector (nullable = true)\n",
    " |-- pc: vector (nullable = true)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45e0f0dc7c75b0c288bbd0f818803065",
     "grade": true,
     "grade_id": "cell-20fb7c865c3ddf0e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(pipe_pca.transform(protein_df).count(), 25)\n",
    "np.testing.assert_equal(type(pipe_pca), PipelineModel)\n",
    "np.testing.assert_equal(len(pipe_pca.transform(protein_df).first().pc), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 pts)\n",
    "\n",
    "Extract the absolute loadings from the PCA transformation in the pipeline and the appropriate feature names from the vector assembler. For each principal component, extract the top feature---the biggest absoluate loadings. Comment on what principal component 1 vs principal component 2 mean based on these top features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "59aa36c0b782325c43cd3225a6363e6e",
     "grade": true,
     "grade_id": "cell-ccff5d0a482e4302",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name       pc1       pc2\n",
      "5  Cereals  0.860865  0.406169\n",
      "\n",
      "   name       pc1       pc2\n",
      "3  Milk  0.425376  0.830856\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "principal_components = pipe_pca.stages[-1].pc.toArray()\n",
    "principal_components\n",
    "\n",
    "x = list(zip(['RedMeat','WhiteMeat', 'Eggs','Milk','Fish','Cereals','Starch','Nuts','FruitVeg'], \n",
    "         principal_components[:, 0], principal_components[:, 1]))\n",
    "pc_loadings = pd.DataFrame(x, columns=['name','pc1','pc2'])\n",
    "\n",
    "a = pc_loadings.apply({'name': lambda x: x, 'pc1':np.abs,'pc2': np.abs}, axis=0)\n",
    "pc_loadings\n",
    "pc1_pd = a.sort_values('pc1', ascending=False)\n",
    "pc2_pd = a.sort_values('pc2', ascending=False)\n",
    "print(pc1_pd.head(1))\n",
    "print()\n",
    "print(pc2_pd.head(1))\n",
    "#Based on two prnicipal components, Cerials is the top feature with largest absoluate loading (0.86) in pc1 \n",
    "#and Milk is the top feature with largest absoluate loading (0.83) in pc1\n",
    "# This means that Cerials and Milk can best represent the dataset. \n",
    "#This means that people from most of the countries in the data set eat cereals or drink milk most of the times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (10 pts)\n",
    "\n",
    "Use the following `exploded_df` dataframe to explore which countries are at the extreme the principal component 1. In particular, create a dataframe `smallest_pc1_df` which contains the columns `country`, `pc_1`, and `pc_2` of the country with the smallest `pc_1`. Similarly, create `largest_pc1_df` with the largest `pc_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ef2748ac9694e621534f919c4698390",
     "grade": false,
     "grade_id": "cell-ef40fee5c7d7bf9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@fn.udf(returnType=types.FloatType())\n",
    "def vector_select(vector_col, element):\n",
    "    return float(vector_col[element])\n",
    "\n",
    "exploded_df = pipe_pca.transform(protein_df).select('country', \n",
    "                                      vector_select('pc', fn.lit(0)).alias('pc_1'),\n",
    "                                      vector_select('pc', fn.lit(1)).alias('pc_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|  country|       pc_1|        pc_2|\n",
      "+---------+-----------+------------+\n",
      "|  Albania|  14.102253|  -1.3218285|\n",
      "|  Austria| -5.4612746|   1.5477921|\n",
      "|  Belgium| -6.0766153|  -1.4793622|\n",
      "| Bulgaria|  26.115946|   3.3188622|\n",
      "|Czechosl.|   3.317268|  -2.0923328|\n",
      "|  Denmark|-13.8607855|   1.3738568|\n",
      "| EGermany| -4.9024506|    -8.35954|\n",
      "|  Finland| -12.262294|   11.290117|\n",
      "|   France|  -6.345336|  0.67163473|\n",
      "|   Greece|   9.036383|   3.0326154|\n",
      "|  Hungary|  10.804568|  -2.3634377|\n",
      "|  Ireland| -11.857348|    5.312214|\n",
      "|    Italy|   6.308735|  -1.3141143|\n",
      "| Netherl.| -11.808532|    2.132773|\n",
      "|   Norway| -11.005402|-0.077100314|\n",
      "|   Poland|  2.5261996|   2.9990442|\n",
      "| Portugal| 0.78424096|  -16.753153|\n",
      "|  Romania|  19.067295|   2.5912552|\n",
      "|    Spain|  1.9230922|  -10.482929|\n",
      "|   Sweden|-14.8419485|   0.7262486|\n",
      "+---------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should contain something similar to this:\n",
    "\n",
    "```\n",
    "exploded_df.show()\n",
    "```\n",
    "\n",
    "```\n",
    "+---------+-----------+------------+\n",
    "|  country|       pc_1|        pc_2|\n",
    "+---------+-----------+------------+\n",
    "|  Albania|  14.102253|  -1.3218285|\n",
    "|  Austria| -5.4612746|   1.5477921|\n",
    "|  Belgium| -6.0766153|  -1.4793622|\n",
    "| Bulgaria|  26.115946|   3.3188622|\n",
    "|Czechosl.|   3.317268|  -2.0923328|\n",
    "|  Denmark|-13.8607855|   1.3738568|\n",
    "| EGermany| -4.9024506|    -8.35954|\n",
    "|  Finland| -12.262294|   11.290117|\n",
    "|   France|  -6.345336|  0.67163473|\n",
    "|   Greece|   9.036383|   3.0326154|\n",
    "|  Hungary|  10.804568|  -2.3634377|\n",
    "|  Ireland| -11.857348|    5.312214|\n",
    "|    Italy|   6.308735|  -1.3141143|\n",
    "| Netherl.| -11.808532|    2.132773|\n",
    "|   Norway| -11.005402|-0.077100314|\n",
    "|   Poland|  2.5261996|   2.9990442|\n",
    "| Portugal| 0.78424096|  -16.753153|\n",
    "|  Romania|  19.067295|   2.5912552|\n",
    "|    Spain|  1.9230922|  -10.482929|\n",
    "|   Sweden|-14.8419485|   0.7262486|\n",
    "+---------+-----------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96d4b13f36029a10d80b2727f2a66dbe",
     "grade": false,
     "grade_id": "cell-e3414da54a6ab4f5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "largest_pc1_df = spark.createDataFrame(exploded_df.orderBy('pc_1', ascending=False).take(1))\n",
    "smallest_pc1_df = spark.createDataFrame(exploded_df.orderBy('pc_1', ascending=True).take(1))\n",
    "#smallest_pc1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6045a3a99e15219280c28400bd802e7c",
     "grade": true,
     "grade_id": "cell-11ee0e6c6a5219e2",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(smallest_pc1_df.count(), 1)\n",
    "np.testing.assert_equal(largest_pc1_df.count(), 1)\n",
    "np.testing.assert_array_less(smallest_pc1_df.first().pc_1, largest_pc1_df.first().pc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 pts)\n",
    "\n",
    "In this question, you will find three clusters of the raw features (without any modification) of `protein_df` using K-means. Do this using a pipeline transformation called `pipe_cluster` which should produce a prediction column called `prediction`. **To reproduce your results, use default parameters of Kmeans and set the `seed` parameter to 0**\n",
    "\n",
    "Produce a dataframe `protein_clustered_df` with the `pipe_cluster` transformation applied to `protein_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1065593910d638d2c98e2d554eaaabeb",
     "grade": false,
     "grade_id": "cell-2b84d6996a843772",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "|  Country|RedMeat|WhiteMeat|Eggs|Milk|Fish|Cereals|Starch|Nuts|FruitVeg|            features|prediction|\n",
      "+---------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "|  Albania|   10.1|      1.4| 0.5| 8.9| 0.2|   42.3|   0.6| 5.5|     1.7|[10.1,1.4,0.5,8.9...|         1|\n",
      "|  Austria|    8.9|     14.0| 4.3|19.9| 2.1|   28.0|   3.6| 1.3|     4.3|[8.9,14.0,4.3,19....|         2|\n",
      "|  Belgium|   13.5|      9.3| 4.1|17.5| 4.5|   26.6|   5.7| 2.1|     4.0|[13.5,9.3,4.1,17....|         2|\n",
      "| Bulgaria|    7.8|      6.0| 1.6| 8.3| 1.2|   56.7|   1.1| 3.7|     4.2|[7.8,6.0,1.6,8.3,...|         1|\n",
      "|Czechosl.|    9.7|     11.4| 2.8|12.5| 2.0|   34.3|   5.0| 1.1|     4.0|[9.7,11.4,2.8,12....|         2|\n",
      "|  Denmark|   10.6|     10.8| 3.7|25.0| 9.9|   21.9|   4.8| 0.7|     2.4|[10.6,10.8,3.7,25...|         0|\n",
      "| EGermany|    8.4|     11.6| 3.7|11.1| 5.4|   24.6|   6.5| 0.8|     3.6|[8.4,11.6,3.7,11....|         2|\n",
      "|  Finland|    9.5|      4.9| 2.7|33.7| 5.8|   26.3|   5.1| 1.0|     1.4|[9.5,4.9,2.7,33.7...|         0|\n",
      "|   France|   18.0|      9.9| 3.3|19.5| 5.7|   28.1|   4.8| 2.4|     6.5|[18.0,9.9,3.3,19....|         2|\n",
      "|   Greece|   10.2|      3.0| 2.8|17.6| 5.9|   41.7|   2.2| 7.8|     6.5|[10.2,3.0,2.8,17....|         1|\n",
      "|  Hungary|    5.3|     12.4| 2.9| 9.7| 0.3|   40.1|   4.0| 5.4|     4.2|[5.3,12.4,2.9,9.7...|         1|\n",
      "|  Ireland|   13.9|     10.0| 4.7|25.8| 2.2|   24.0|   6.2| 1.6|     2.9|[13.9,10.0,4.7,25...|         0|\n",
      "|    Italy|    9.0|      5.1| 2.9|13.7| 3.4|   36.8|   2.1| 4.3|     6.7|[9.0,5.1,2.9,13.7...|         2|\n",
      "| Netherl.|    9.5|     13.6| 3.6|23.4| 2.5|   22.4|   4.2| 1.8|     3.7|[9.5,13.6,3.6,23....|         0|\n",
      "|   Norway|    9.4|      4.7| 2.7|23.3| 9.7|   23.0|   4.6| 1.6|     2.7|[9.4,4.7,2.7,23.3...|         0|\n",
      "|   Poland|    6.9|     10.2| 2.7|19.3| 3.0|   36.1|   5.9| 2.0|     6.6|[6.9,10.2,2.7,19....|         2|\n",
      "| Portugal|    6.2|      3.7| 1.1| 4.9|14.2|   27.0|   5.9| 4.7|     7.9|[6.2,3.7,1.1,4.9,...|         2|\n",
      "|  Romania|    6.2|      6.3| 1.5|11.1| 1.0|   49.6|   3.1| 5.3|     2.8|[6.2,6.3,1.5,11.1...|         1|\n",
      "|    Spain|    7.1|      3.4| 3.1| 8.6| 7.0|   29.2|   5.7| 5.9|     7.2|[7.1,3.4,3.1,8.6,...|         2|\n",
      "|   Sweden|    9.9|      7.8| 3.5|24.7| 7.5|   19.5|   3.7| 1.4|     2.0|[9.9,7.8,3.5,24.7...|         0|\n",
      "+---------+-------+---------+----+----+----+-------+------+----+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "pipe_cluster = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols=['RedMeat','WhiteMeat', 'Eggs','Milk','Fish','Cereals','Starch','Nuts','FruitVeg'],\n",
    "                            outputCol='features')\n",
    "    , clustering.KMeans(k=3, featuresCol='features', predictionCol='prediction', seed=0)]).fit(protein_df)\n",
    "\n",
    "protein_clustered_df = pipe_cluster.transform(protein_df)\n",
    "protein_clustered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfb9762f17b64f2d9ce5ed591937cc22",
     "grade": true,
     "grade_id": "cell-d352a2be7c9d703f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 pts\n",
    "np.testing.assert_equal(type(pipe_cluster), PipelineModel)\n",
    "np.testing.assert_equal(len(pipe_cluster.stages), 2)\n",
    "np.testing.assert_equal(protein_clustered_df.count(), 25)\n",
    "assert 'prediction' in protein_clustered_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 pts)\n",
    "\n",
    "According to Wikipedia, the Mediterranean diet includes \"proportionally high consumption of olive oil, legumes, unrefined cereals, fruits, and vegetables, moderate to high consumption of fish, moderate consumption of dairy products (mostly as cheese and yogurt), moderate wine consumption, and low consumption of non-fish meat products.\" https://en.wikipedia.org/wiki/Mediterranean_diet\n",
    "\n",
    "We have some of these countries in our protein dataset: Italy, Spain, Portugal, and Greece.\n",
    "\n",
    "With code and code comments, test the hypothesis that these countries all belong to the same cluster using the clustering you did in question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the following list in your code\n",
    "mediterranean_countries = ['Italy', 'Spain', 'Portugal', 'Greece']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "415e2f135d2fffb38e3a4f04cb8b4c36",
     "grade": true,
     "grade_id": "cell-9285cef3ce344449",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| country|prediction|\n",
      "+--------+----------+\n",
      "|   Italy|         2|\n",
      "|   Spain|         2|\n",
      "|Portugal|         2|\n",
      "|  Greece|         1|\n",
      "+--------+----------+\n",
      "\n",
      "+-------------------------+----------+\n",
      "|distinct_prediction_count|prediction|\n",
      "+-------------------------+----------+\n",
      "|                        1|         1|\n",
      "|                        3|         2|\n",
      "+-------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('spark-intro').getOrCreate()\n",
    "\n",
    "#select country and prediction from protein_clustered_df and store it into SQL table:\n",
    "\n",
    "protein_filteredData = protein_clustered_df.select('country','prediction')\n",
    "#protein_filteredData.show()\n",
    "sqlContext.registerDataFrameAsTable(protein_filteredData, \"Data\")\n",
    "\n",
    "# Convert the list of mediterranean countries into SQL table:\n",
    "sampleContries_DF = spark.createDataFrame(list(map(lambda x: Row(country=x), mediterranean_countries)))\n",
    "sqlContext.registerDataFrameAsTable(sampleContries_DF, \"sampleCountries\")\n",
    "\n",
    "#Identify the prediction cluster of each mediterranean country by comparing it with protein_clustered_df using SQL join query \n",
    "#spark.sql(\"select * from sampleCountries\").show()\n",
    "output1 = spark.sql(\"select d.country, d.prediction from Data d join  sampleCountries c on d.country = c.country\")\n",
    "sqlContext.registerDataFrameAsTable(output1, \"results\")\n",
    "spark.sql(\"select * from results\").show()\n",
    "\n",
    "#Display distinct prediction clusters for mediterranean countries\n",
    "spark.sql(\"select count(*) as distinct_prediction_count, prediction from results group by prediction\").show()\n",
    "\n",
    "# The countries consist of 2 different clusters i.e. cluster number 1(with Greece) and 2 (with Italy, Spain and Portugal).\n",
    "#This proves the hypothesis wrong of all the countries being in the same cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
